{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250f541-92ac-46b0-a1d4-dbace90edebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7a2bb-95b0-47b0-9a22-0fc0a7f9dfaa",
   "metadata": {},
   "source": [
    "# Scraping data from wikipedia\n",
    "We must first define the url that we will scrap data from. They will allow us to make a mapping between books and their film adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b7c47-14ce-4ca7-b017-d5b4ad709a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Wikipedia page\n",
    "url_0_C = \"https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(0%E2%80%939,_A%E2%80%93C)\"\n",
    "url_D_J = \"https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(D%E2%80%93J)\"\n",
    "url_K_R = \"https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(K%E2%80%93R)\"\n",
    "url_S_Z = \"https://en.wikipedia.org/wiki/List_of_fiction_works_made_into_feature_films_(S%E2%80%93Z)\"\n",
    "url_short = \"https://en.wikipedia.org/wiki/List_of_short_fiction_made_into_feature_films\"\n",
    "url_kids = \"https://en.wikipedia.org/wiki/List_of_children%27s_books_made_into_feature_films\"\n",
    "\n",
    "urls = [url_0_C, url_D_J, url_K_R, url_S_Z, url_short, url_kids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18c99c-593d-4829-84e2-e14d4a8fd3f6",
   "metadata": {},
   "source": [
    "Then, we will define a series of functions that we will use when scraping and processing its result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c1943-31be-4cba-9f7a-b5f31de05e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraps a url to extract a list of fiction works and their film adaptation\n",
    "def scrap_book_to_movie(url): \n",
    "    response = requests.get(url)\n",
    "    result = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        tables = content.find_all('table', {'class': 'wikitable'})\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "\n",
    "                # splits into book and movie\n",
    "                cell_tab = [cell.get_text(strip=True) for cell in cells]\n",
    "                result.append(cell_tab)    \n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "    result = pd.DataFrame(result)\n",
    "    result.columns = ['fiction_work', 'film_adaptations']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228536be-ebec-4cbe-9708-b9385fc7b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts years in format \"2000\", \"1999-2000\" or \"1999-present\"\n",
    "def extract_years(text):\n",
    "    years = re.findall(r'\\((?:[^)]*?)(\\d{4}(?:[–-](?:\\d{4}|present))?)(?:[^)]*?)\\)', text)\n",
    "    return years[0] if years else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483303e-0afa-4ed9-91ba-988c5461d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup authors feature\n",
    "def clean_authors(authors): \n",
    "    authors = authors.fillna(\"\")\n",
    "    authors = authors.replace(\"unknown\", \"\")\n",
    "\n",
    "    authors = authors.apply(lambda a: re.sub(r'and([A-Z])', r', \\1', a))\n",
    "    authors = authors.apply(lambda a: a.replace('and ', ','))\n",
    "\n",
    "    authors = authors.apply(lambda a: a.replace('(series)', ''))\n",
    "    authors = authors.apply(lambda a: a.replace('various authors', ''))\n",
    "    \n",
    "    authors = authors.apply(lambda a: re.sub(r'\\[.*?\\]', '', a))\n",
    "    authors = authors.apply(lambda a: re.sub(r'\\(as[^)]+\\)', '', a))\n",
    "    authors = authors.apply(lambda a: re.sub(r'\\(pseudonym[^)]+\\)', '', a))\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844f161-ec80-4b2f-bd34-78def8b68c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts authors from a text\n",
    "def extract_authors(text): \n",
    "    if any(substring in text for substring in [\" fils\", \" père\", \" III\", \" Sr.\", \" Jr.\"]):\n",
    "        text[-1] = text[-1].replace(r'\\[.*?\\]', '')\n",
    "        return \",\".join(text[-2:])\n",
    "    if len(text) > 1 :\n",
    "        return text[-1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78729e-55b0-49c8-9dc9-f78dcb3239d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts several features from a dataframe while sanitizing them\n",
    "def extract_features(df):\n",
    "    df['title_book'] = df['fiction_work'].str.split('(').str[0]\n",
    "    df['title_book'] = df['title_book'].apply(lambda t: t.replace('\"', ''))\n",
    "\n",
    "    df_split_comma = df['fiction_work'].str.split(',')\n",
    "    df['author_book'] = df_split_comma.apply(extract_authors)\n",
    "    df['author_book'] = clean_authors(df['author_book'])\n",
    "\n",
    "    df['year_book'] = df['fiction_work'].apply(extract_years)\n",
    "\n",
    "    df['title_film'] = df['film_adaptations'].str.split('(').str[0]\n",
    "    df['year_film'] = df['film_adaptations'].apply(extract_years)\n",
    "\n",
    "    df = df.drop(['fiction_work', 'film_adaptations'], axis = 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f2c2d-d6a7-4583-add8-765d1b284d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in values \"same as above\" and \"same as below\" with the data above or below respectively\n",
    "def clean_same_as_above_below(df):\n",
    "    indexes = df.index[df['title_film'] == 'same as above'].tolist()\n",
    "    target_ind =[(index - 1) for index in indexes]\n",
    "    df['title_film'][indexes] = df['title_film'][target_ind]\n",
    "    df['year_film'][indexes] = df['year_film'][target_ind]\n",
    "\n",
    "    indexes = df.index[df['title_film'] == 'same as below'].tolist()\n",
    "    target_ind =[(index + 1) for index in indexes]\n",
    "    df['title_film'][indexes] = df['title_film'][target_ind]\n",
    "    df['year_film'][indexes] = df['year_film'][target_ind]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bf4e4-1e53-43ce-aeaa-f8611a22f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final processing on scrapping result - drops nan, null columns, fills empty rows and cleans the features\n",
    "def scrap_post_processing(df): \n",
    "    df.loc[df['film_adaptations'].isnull() & df['fiction_work'].notnull(), ['film_adaptations']] = df['fiction_work']\n",
    "    df.loc[df['film_adaptations'] == df['fiction_work'], ['fiction_work']] = None\n",
    "\n",
    "    # fill nan fiction_work values with the last non null value of fiction_work\n",
    "    df['fiction_work'] = df['fiction_work'].ffill()\n",
    "    # drop nan where both columns are nan\n",
    "    df = df.dropna(subset=['film_adaptations'])\n",
    "\n",
    "    df = extract_features(df)\n",
    "\n",
    "    df = clean_same_as_above_below(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e631e-07cd-4afd-adf1-559944accf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launches the scrapping on every url selected\n",
    "dataframes = []\n",
    "for url in urls: \n",
    "    df = scrap_book_to_movie(url)\n",
    "    clean_df = scrap_post_processing(df)\n",
    "    dataframes.append(clean_df)\n",
    "\n",
    "book_adaptations = pd.concat(dataframes).reset_index(drop=True)\n",
    "book_adaptations = book_adaptations.drop_duplicates().reset_index(drop=True)\n",
    "book_adaptations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b936c-d0b6-4913-bff4-9a94acdc1a50",
   "metadata": {},
   "source": [
    "We now have a dataframe with 3549 film adaptations together with the book they are adapting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560729e",
   "metadata": {},
   "source": [
    "# Merge with Goodreads\n",
    "We will now merge the book to movie mapping with the goodreads dataset to have additional information on the books.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66fcfa",
   "metadata": {},
   "source": [
    "First we download the dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"bahramjannesarr/goodreads-book-datasets-10m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def books_csv_to_df(path):\n",
    "    book_csv_list = os.listdir(path)[:-1]\n",
    "    book_csv_path_list = [os.path.join(path, book_csv) for book_csv in book_csv_list]\n",
    "    dataframes = []\n",
    "    for path, name in zip(book_csv_path_list, book_csv_list):\n",
    "        df = pd.read_csv(path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    df_goodreads = pd.concat(dataframes)\n",
    "    return df_goodreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(column):\n",
    "    return column.apply(lambda name: str(name).lower().replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc027b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parenthesis(column):\n",
    "    return column.apply(lambda name: re.sub(r\"\\(.*?\\)\", \"\", str(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9db444",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"..\\data\"\n",
    "\n",
    "df_movies = book_adaptations.copy()\n",
    "df_goodreads = books_csv_to_df(path)\n",
    "\n",
    "df_goodreads['merge_authors'] = clean_spaces(df_goodreads['Authors'])\n",
    "df_goodreads['merge_names'] = clean_spaces(df_goodreads['Name'])\n",
    "df_goodreads['merge_names'] = remove_parenthesis(df_goodreads['merge_names'])\n",
    "\n",
    "\n",
    "df_movies['merge_authors'] = clean_spaces(df_movies['author_book'])\n",
    "df_movies['merge_names'] = clean_spaces(df_movies['title_book'])\n",
    "df_movies['merge_names'] = remove_parenthesis(df_movies['merge_names'])\n",
    "\n",
    "merge_goodreads = df_goodreads.merge(right=df_movies, how=\"right\", left_on=['merge_authors', 'merge_names'], right_on=['merge_authors', 'merge_names'], copy=False)\n",
    "merge_goodreads = merge_goodreads.drop_duplicates(subset = df_movies.columns).reset_index(drop=True)\n",
    "merge_goodreads = merge_goodreads.drop(columns = ['merge_authors', 'merge_names', 'Authors', 'Name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce9a90-1e29-4044-8ccc-952c173ac066",
   "metadata": {},
   "source": [
    "# Merge with CMU\n",
    "We will now merge this data with the CMU dataset to add extra information on these films.\n",
    "\n",
    "First, we define a function that we will use to clean titles and compare them consistently when merging datasets based on the film's title. We also define any function used for cleaning later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ce315-b88e-4424-a01d-33e98c211a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup of titles consistent accross datasets\n",
    "def clean_title(title):\n",
    "    return title.lower().replace(\" \", \"\")\n",
    "\n",
    "# clean a json-like representation into a list of the values of the json-pairs\n",
    "def clean_json_format(list):\n",
    "    dict = ast.literal_eval(list)\n",
    "    return ', '.join(dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc84150-b5d4-4c8d-bc3a-f72347275ab0",
   "metadata": {},
   "source": [
    "Then we can proceed with the merging with CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a525c1-650b-4b04-81bc-7204d4d21a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Merge df with CMU depending on title_film and year_film\n",
    "def merge_with_CMU(df):    \n",
    "    cmu_movies = pd.read_csv(\"../MovieSummaries/movie.metadata.tsv\", sep='\\t', usecols=[2,3,4,5,6,7,8], names=['movie_name', 'movie_date', 'box_office', 'runtime', 'language', 'countries', 'genres'])\n",
    "    \n",
    "    # Clean the move name and place it in another column to save the original version. This will also be used when merging with IMDB\n",
    "    name_column = cmu_movies['movie_name']\n",
    "    new_name_column = name_column.apply(clean_title)\n",
    "    cmu_movies['clean_name'] = new_name_column\n",
    "\n",
    "    # Clean the title from the dataset scrapped to allow merge on title with CMU\n",
    "    title_column = df['title_film']\n",
    "    new_title_column = title_column.apply(clean_title)\n",
    "    df['title_film'] = new_title_column\n",
    "\n",
    "    # Clean json values to have a nicer representation\n",
    "    cmu_movies['language'] = cmu_movies['language'].apply(clean_json_format)\n",
    "    cmu_movies['countries'] = cmu_movies['countries'].apply(clean_json_format)\n",
    "    cmu_movies['genres'] = cmu_movies['genres'].apply(clean_json_format)\n",
    "    \n",
    "    # Extract only the year of the Movie release date\n",
    "    date_column = cmu_movies['movie_date']\n",
    "    new_date_column = date_column.apply(lambda x : str(x)[0:4])\n",
    "    cmu_movies['movie_date'] = new_date_column\n",
    "    \n",
    "    # Do the merge\n",
    "    merge_cmu = cmu_movies.merge(right=df, how=\"inner\", left_on=['clean_name', 'movie_date'], right_on=['title_film', 'year_film'], copy=False)\n",
    "    merge_cmu = merge_cmu.drop(['title_film', 'year_film'], axis=1)\n",
    "    merge_cmu = merge_cmu.drop_duplicates()\n",
    "    merge_cmu = merge_cmu.dropna(subset=['movie_date'])\n",
    "    merge_cmu['movie_date'] = merge_cmu['movie_date'].astype('int64')\n",
    "    return merge_cmu\n",
    "\n",
    "merge_cmu = merge_with_CMU(merge_goodreads)\n",
    "merge_cmu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adae965-c3a9-438c-a509-35f2d03e6e6d",
   "metadata": {},
   "source": [
    "Now we have more information on the films that are an adaptation of a book, such as their genres. Let's add more information such as the film's rating by merging with IMDB's dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb587c-f2b7-46c6-b73f-e009c90aaf3b",
   "metadata": {},
   "source": [
    "# Merge with IMDB\n",
    "The merging with IMDB takes place in two steps. First, we must merge with *title.basics.tsv* to add IMDB's *titleId* feature to each film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46950b-549f-4e77-9bfc-ebb0b513703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/\"\n",
    "\n",
    "# merge a dataframe with 'title.basics.tsv' and append 'imdbID' and 'isAdult' as new features\n",
    "def merge_with_imdb_id(df):\n",
    "    titles = pd.read_csv(data_folder + \"title.basics.tsv\", sep='\\t', header=0, usecols=[0, 1, 2, 3, 4, 5], names=['imdbID', 'titleType', 'imdbPrimaryTitle', 'imdbOriginalTitle', 'isAdult', 'imdbYear'], dtype={'isAdult': 'string'})\n",
    "\n",
    "    # sanitize imdbYear, isAdult and titleType\n",
    "    titles['imdbYear'] = pd.to_numeric(titles['imdbYear'], errors='coerce')\n",
    "    titles['isAdult'] = pd.to_numeric(titles['isAdult'], errors='coerce')\n",
    "    titles = titles.dropna()\n",
    "    titles = titles[titles['titleType'] == 'movie']\n",
    "\n",
    "    # merge with both the imdb's original and primary titles to consider both cases\n",
    "    titles['imdbOriginalTitle'] = titles['imdbOriginalTitle'].apply(clean_title)\n",
    "    titles['imdbPrimaryTitle'] = titles['imdbPrimaryTitle'].apply(clean_title)\n",
    "    mergeOnOriginal = pd.merge(titles, df, how='inner', left_on=['imdbOriginalTitle', 'imdbYear'], right_on=['clean_name', 'movie_date'])\n",
    "    mergeOnPrimary = pd.merge(titles, df, how='inner', left_on=['imdbPrimaryTitle', 'imdbYear'], right_on=['clean_name', 'movie_date'])\n",
    "    \n",
    "    merge = pd.concat([mergeOnOriginal, mergeOnPrimary], axis=0)\n",
    "    merge = merge.drop_duplicates(subset=['imdbID'])\n",
    "    merge = merge.drop(['imdbPrimaryTitle', 'imdbOriginalTitle', 'imdbYear', 'titleType'], axis=1)\n",
    "    \n",
    "    return merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb5479-ec8b-4c1f-bece-f29fdbcbd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge a dataframe with 'title.ratings.tsv' to add rating and number of votes\n",
    "def merge_with_imdb_ratings(df):\n",
    "    imdb_ratings = pd.read_csv(data_folder + \"title.ratings.tsv\", sep='\\t', header=0, names=['imdbID', 'rating', 'numVotes'])\n",
    "\n",
    "    # sanitize rating and numVotes\n",
    "    imdb_ratings['rating'] = pd.to_numeric(imdb_ratings['rating'])\n",
    "    imdb_ratings['numVotes'] = pd.to_numeric(imdb_ratings['numVotes'])\n",
    "    imdb_ratings = imdb_ratings.dropna()\n",
    "\n",
    "    # merge using the common imdbID\n",
    "    return pd.merge(df, imdb_ratings, how='inner', on=['imdbID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66503eea-6daf-474c-9f1c-520bdf180f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge a dataframe with IMDB's datasets\n",
    "def merge_with_imdb(df):\n",
    "    # perform merging\n",
    "    merged_id = merge_with_imdb_id(df)\n",
    "    merge_imdb = merge_with_imdb_ratings(merged_id)\n",
    "\n",
    "    # cleanup merging\n",
    "    merge_imdb = merge_imdb.drop(['imdbID', 'clean_name'], axis=1)\n",
    "    \n",
    "    print('lines dropped during merge with IMDB: ', len(df) - len(merge_imdb))\n",
    "    return merge_imdb\n",
    "\n",
    "merge_imdb = merge_with_imdb(merge_cmu)\n",
    "merge_imdb.to_csv('merge_imdb.csv', index=False)\n",
    "merge_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a66bdc-541c-49bf-8fa1-de4cb2c86387",
   "metadata": {},
   "source": [
    "We now have 1940 film samples that are adaptations from known books and which can use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47afe5e-2db6-45fc-a0fd-105e4c681e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
